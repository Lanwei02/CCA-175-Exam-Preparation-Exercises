// question 1
sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-textfile \
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
--target-dir /user/lanwei94/problem4/text \
-m 1


// question 2
sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-avrodatafile \
--target-dir /user/lanwei94/problem4/avro \
-m 1


// question 3:
sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-parquetfile \
--target-dir /user/lanwei94/problem4/parquet \
-m 1


// question 4:

import com.databricks.spark.avro._

// 1 >
spark-shell --master yarn \
--conf spark.ui.port=12345 \
--packages com.databricks:spark-avro_2.10:2.0.1

val orders1 = sqlContext.read.avro("/user/lanwei94/problem4/avro")

sqlContext.setConf("spark.sql.parquet.Compression.codec", "sanppy")
orders1.write.parquet("/user/lanwei94/problem4/parquet-snappy-compress")

// 2 >
val orders2 = sqlContext.read.avro("/user/lanwei94/problem4/avro").map(rec => rec(0)+"\t"+rec(1)+"\t"+rec(2)+"\t"+rec(3)).saveAsTextFile("/user/lanwei94/problem4/text-gzip-compress", classOf[org.apache.hadoop.io.compress.GzipCodec])

// 3 >
val orders3 = sqlContext.read.avro("/user/lanwei94/problem4/avro").map(rec => (rec(0).toString, rec(0)+"\t"+rec(1)+"\t"+rec(2)+"\t"+rec(3)))
orders3.saveAsSequenceFile("/user/lanwei94/problem4/sequence")

// 4 >
val orders4 = sqlContext.read.avro("/user/lanwei94/problem4/avro").map(rec => rec(0)+"\t"+rec(1)+"\t"+rec(2)+"\t"+rec(3)).saveAsTextFile("/user/lanwei94/problem4/text-snappy-compress", classOf[org.apache.hadoop.io.compress.SnappyCodec])

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--target-dir /user/lanwei94/problem4/text-snappy-compress2 \
--as-textfile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1

// question 5:
// 1 >
val data = sqlContext.read.parquet("/user/lanwei94/problem4/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec", "uncompressed")
data.write.parquet("/user/lanwei94/problem4/parquet-no-compress")


// 2 >
import com.databricks.spark.avro._

val data2 = sqlContext.read.parquet("/user/lanwei94/problem4/parquet-snappy-compress")
sqlContext.setConf("saprk.sql.avro.compression.codec", "snappy")
data2.write.avro("/user/lanwei94/problem4/avro-snappy")


// question 6:
// 1 >
import com.databricks.spark.avro._
val data = sqlContext.read.avro("/user/lanwei94/problem4/avro-snappy")

data.toJSON.saveAsTextFile("/user/lanwei94/problem4/json-no-compress")

// 2 >
val data = sqlContext.read.avro("/user/lanwei94/problem4/avro-snappy")
data.toJSON.saveAsTextFile("/user/lanwei94/problem4/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])

// question 7:
var jsonData = sqlContext.read.json("/user/lanwei94/problem4/json-gzip")
jsonData.map(rec => rec(0)+","+rec(1)+","+rec(2)+","+rec(3)).saveAsTextFile("/user/lanwei94/problem4/csv-gzip", classOf[org.apache.hadoop.io.compress.GzipCodec])

// question 8:
